"""CVE Exploit链接分析技能

提供通用的exploit链接内容分析功能，支持从不同类型的源头提取CVE复现信息。
所有功能专注于防御性安全分析。
"""

import re
from typing import Dict, List, Optional, Any
from urllib.parse import urlparse


def analyze_exploit_links(
    cve_id: str,
    exploit_links: List[str],
    use_webfetch: bool = True
) -> Dict[str, Any]:
    """分析CVE exploit链接，提取复现相关信息
    
    Args:
        cve_id: CVE编号 (如 "CVE-2018-17942")
        exploit_links: exploit链接列表
        use_webfetch: 是否使用WebFetch进行实际内容分析
        
    Returns:
        Dict: 结构化的exploit分析结果
    """
    analysis_result = {
        'cve_id': cve_id,
        'total_links': len(exploit_links),
        'link_analysis': [],
        'extracted_info': {
            'trigger_conditions': [],
            'test_cases': [],
            'compilation_flags': [],
            'environment_requirements': [],
            'reproduction_steps': [],
            'technical_details': []
        },
        'recommendations': []
    }
    
    for link in exploit_links:
        link_info = _classify_and_analyze_link(link, cve_id, use_webfetch)
        analysis_result['link_analysis'].append(link_info)
        
        # 合并提取的信息
        _merge_extracted_info(analysis_result['extracted_info'], link_info.get('extracted_info', {}))
    
    # 生成基于分析结果的建议
    analysis_result['recommendations'] = _generate_reproduction_recommendations(
        analysis_result['extracted_info']
    )
    
    return analysis_result


def _classify_and_analyze_link(link: str, cve_id: str, use_webfetch: bool) -> Dict[str, Any]:
    """分类并分析单个exploit链接"""
    
    parsed_url = urlparse(link)
    domain = parsed_url.netloc.lower()
    path = parsed_url.path.lower()
    
    link_info = {
        'url': link,
        'type': 'unknown',
        'priority': 'low',
        'status': 'pending',
        'extracted_info': {}
    }
    
    # 根据URL特征分类链接类型
    if 'github.com' in domain or 'gitlab.com' in domain:
        if '/commit/' in path:
            link_info['type'] = 'code_commit'
            link_info['priority'] = 'high'
        elif '/issues/' in path or '/pull/' in path:
            link_info['type'] = 'issue_tracker'
            link_info['priority'] = 'medium'
    elif 'lists.' in domain or 'archive' in domain:
        link_info['type'] = 'mailing_list'
        link_info['priority'] = 'high'
    elif 'savannah.' in domain or 'bugs?' in path:
        link_info['type'] = 'bug_report'
        link_info['priority'] = 'high'
    elif 'security' in domain or 'advisory' in path:
        link_info['type'] = 'security_advisory'
        link_info['priority'] = 'medium'
    
    if use_webfetch:
        # 为不同类型的链接生成特定的分析prompt
        prompt = _generate_analysis_prompt(link_info['type'], cve_id)
        link_info['analysis_prompt'] = prompt
        link_info['extracted_info'] = _extract_info_from_content(link, prompt, link_info['type'])
        link_info['status'] = 'analyzed'
    
    return link_info


def _generate_analysis_prompt(link_type: str, cve_id: str) -> str:
    """为不同类型的exploit链接生成分析prompt"""
    
    base_prompt = f"分析与{cve_id}相关的技术信息，重点提取："
    
    if link_type == 'code_commit':
        return f"""{base_prompt}
1. 代码修复的具体内容和位置
2. 测试用例的实现细节
3. 编译选项和构建要求
4. 漏洞触发的具体函数和参数
5. 补丁前后的代码差异分析
专注于可用于复现漏洞的技术细节。"""
        
    elif link_type == 'mailing_list':
        return f"""{base_prompt}
1. 漏洞发现者的原始报告和复现方法
2. 具体的触发条件和测试输入
3. 编译环境和运行时要求
4. 社区讨论中的技术细节和解决方案
5. 任何提到的测试命令或脚本
重点关注实际的复现步骤和技术讨论。"""
        
    elif link_type == 'bug_report':
        return f"""{base_prompt}
1. 详细的问题复现步骤
2. 环境配置和依赖要求
3. 具体的错误信息和症状
4. 测试用例和输入样本
5. 修复状态和解决方案
专注于可操作的复现指导信息。"""
        
    elif link_type == 'security_advisory':
        return f"""{base_prompt}
1. 官方的漏洞描述和影响范围
2. 推荐的测试和验证方法
3. 缓解措施和修复建议
4. 技术细节和根本原因分析
5. 相关的安全评估信息
重点提取技术分析和验证方法。"""
        
    else:
        return f"""{base_prompt}
1. 任何与漏洞复现相关的技术细节
2. 测试方法、输入样本、触发条件
3. 环境要求、编译选项、依赖配置
4. 错误信息、崩溃现象、分析工具使用
5. 修复方案、补丁信息、安全建议
提取所有可用于漏洞复现的实用信息。"""


def _extract_info_from_content(link: str, prompt: str, link_type: str) -> Dict[str, Any]:
    """从链接内容中提取信息 (模拟WebFetch调用)"""
    
    # 这里应该是实际的WebFetch调用
    # 为了演示，返回结构化的提取信息模板
    extracted_info = {
        'trigger_conditions': [],
        'test_cases': [],
        'compilation_flags': [],
        'environment_requirements': [],
        'reproduction_steps': [],
        'technical_details': []
    }
    
    # 实际实现中，这里会调用WebFetch并解析返回的内容
    # content = WebFetch(url=link, prompt=prompt)
    # extracted_info = _parse_webfetch_response(content, link_type)
    
    return extracted_info


def _merge_extracted_info(target: Dict[str, List], source: Dict[str, List]) -> None:
    """合并提取的信息到目标字典中"""
    for key in target:
        if key in source:
            # 去重合并
            existing_items = set(target[key])
            for item in source[key]:
                if item not in existing_items:
                    target[key].append(item)


def _generate_reproduction_recommendations(extracted_info: Dict[str, List]) -> List[str]:
    """基于提取的信息生成复现建议"""
    recommendations = []
    
    # 基于提取信息的数量和质量生成建议
    if extracted_info['trigger_conditions']:
        recommendations.append(
            f"发现 {len(extracted_info['trigger_conditions'])} 个触发条件，"
            "建议优先使用这些条件进行复现测试"
        )
    
    if extracted_info['compilation_flags']:
        recommendations.append(
            f"检测到 {len(extracted_info['compilation_flags'])} 个编译选项，"
            "建议按照这些选项构建测试环境"
        )
    
    if extracted_info['test_cases']:
        recommendations.append(
            f"找到 {len(extracted_info['test_cases'])} 个测试用例，"
            "建议作为AFL++的初始种子进行进一步fuzzing"
        )
    
    if not any(extracted_info.values()):
        recommendations.append(
            "未能从exploit链接中提取到足够的技术细节，"
            "建议手动分析链接内容或寻找其他exploit资源"
        )
    
    # 通用建议（总是添加，确保至少有一些建议）
    recommendations.extend([
        "建议使用AddressSanitizer等调试工具验证复现结果",
        "确保在隔离环境中进行安全测试",
        "记录复现过程以便后续分析和报告"
    ])
    
    # 确保始终至少有一个建议
    if not recommendations:
        recommendations.append("建议进一步分析目标程序的安全特征")
    
    return recommendations


def classify_exploit_link_priority(link: str) -> str:
    """对exploit链接进行优先级分类
    
    Args:
        link: exploit链接URL
        
    Returns:
        str: 优先级 ('high', 'medium', 'low')
    """
    link_lower = link.lower()
    
    # 高优先级：代码提交、邮件列表、bug报告
    high_priority_indicators = [
        'github.com', 'gitlab.com', 'commit',
        'lists.', 'archive', 'mailing',
        'savannah.', 'bugs', 'bugzilla',
        'func=detailitem'  # 专门匹配savannah bug链接格式
    ]
    
    # 中优先级：安全公告、问题追踪
    medium_priority_indicators = [
        'security', 'advisory', 'issues', 'pull'
    ]
    
    if any(indicator in link_lower for indicator in high_priority_indicators):
        return 'high'
    elif any(indicator in link_lower for indicator in medium_priority_indicators):
        return 'medium'
    else:
        return 'low'


def extract_cve_links_from_page(cve_page_content: str) -> List[str]:
    """从CVE页面内容中提取exploit相关链接
    
    Args:
        cve_page_content: CVE页面的HTML内容
        
    Returns:
        List[str]: 提取到的链接列表
    """
    # 使用正则表达式提取链接
    # 这里应该根据实际的CVE页面结构进行调整
    url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+'
    links = re.findall(url_pattern, cve_page_content)
    
    # 过滤相关的链接
    relevant_links = []
    exploit_keywords = ['exploit', 'proof-of-concept', 'poc', 'reproduction']
    
    for link in links:
        # 优先选择明确标记为exploit的链接
        if any(keyword in link.lower() for keyword in exploit_keywords):
            relevant_links.append(link)
        # 也包括可能包含技术细节的其他链接
        elif any(domain in link.lower() for domain in ['github.com', 'lists.', 'savannah.', 'bugs']):
            relevant_links.append(link)
    
    # 去重并返回
    return list(set(relevant_links))


# 导出的公共函数
__all__ = [
    'analyze_exploit_links',
    'classify_exploit_link_priority', 
    'extract_cve_links_from_page'
]